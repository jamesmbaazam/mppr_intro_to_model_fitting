{
  "hash": "db1bf8a7f3236fc402c86346fdd91445",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Model Fitting and Calibration\"\nsubtitle: \"Modelling for Pandemic Preparedness and Response Modular Shortcourse, 2025\"\nauthor:\n  - name: \"James Mba Azam, PhD\"\n    orcid: 0000-0001-5782-7330\n    email: james.azam@lshtm.ac.uk\n    affiliation:\n      - name: London School of Hygiene and Tropical Medicine, UK\n        city: London, United Kingdom\ndate: \"last-modified\"\nlicense: \"CC BY\"\ncopyright:\n  holder: James Mba Azam\n  year: 2025\nexecute:\n  echo: true\n  cache: true\n  message: false\n  warning: false\n  freeze: auto\nformat:\n  revealjs:\n    theme: [dark]\n    slide-number: true\n    scrollable: true\n    chalkboard: true\n    toc: true\n    toc-depth: 1\n    transition: fade\n    lightbox: true\n    progress: true\n    code-copy: true\n    code-annotations: hover\n    code-overflow: wrap\n    code-line-numbers: true\n    code-block-bg: true \n  beamer:\n    theme: metropolis\n    code-annotations: hover\n# bibliography: references.bib\nlink-citations: true\nengine: knitr\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n## Learning Objectives\n\nBy the end of this lecture, you will be able to:\n\n-   Understand the <span style=\"color: tomato;\">fundamental concepts</span> of <span style=\"color: tomato;\">model fitting</span> and <span style=\"color: tomato;\">calibration</span>\n-   Apply <span style=\"color: tomato;\">least squares estimation</span> to <span style=\"color: tomato;\">compartmental models</span>\n-   Implement <span style=\"color: tomato;\">maximum likelihood estimation</span> for <span style=\"color: tomato;\">epidemic models</span>\n-   Compare the <span style=\"color: tomato;\">strengths and weaknesses</span> of different <span style=\"color: tomato;\">fitting methods</span>\n-   Recognize when to use <span style=\"color: tomato;\">advanced methods</span> like <span style=\"color: tomato;\">MCMC</span> and <span style=\"color: tomato;\">particle filtering</span>\n-   <span style=\"color: tomato;\">Troubleshoot</span> common <span style=\"color: tomato;\">fitting problems</span>\n\n\n# Introduction & Motivation\n\n------------------------------------------------------------------------\n\n## Why Model Fitting Matters\n\n### The Challenge\n\n**The Challenge:**\n\n-   We have mathematical models (e.g., SIR, SEIR)\n-   We have real-world data (case counts, hospitalizations)\n-   **<span style=\"color: tomato;\">How do we connect them?</span>**\n\n------------------------------------------------------------------------\n\n### The Goal\n\n-   <span style=\"color: #FFFF00;\">Find parameter values that make our model predictions match observed data</span>\n-   <span style=\"color: #FFFF00;\">Quantify uncertainty in our estimates</span>\n-   <span style=\"color: #FFFF00;\">Make reliable predictions and policy recommendations</span>\n\n------------------------------------------------------------------------\n\n## Example: Outbreak data\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/covid-example-1.png){width=960}\n:::\n:::\n\n\n\n**<span style=\"color: tomato;\">Question:</span>** How do we estimate $\\beta$ and $\\gamma$ from this noisy data?\n\n## Types of Fitting Methods\n\n### Deterministic Methods\n\n-   **<span style=\"color: tomato;\">Least Squares</span>**\n-   **<span style=\"color: tomato;\">Maximum Likelihood Estimation (MLE)</span>**\n\n------------------------------------------------------------------------\n\n### Stochastic Methods\n\n-   **<span style=\"color: #FFFF00;\">Markov Chain Monte Carlo (MCMC)</span>**\n-   **<span style=\"color: #FFFF00;\">Sequential Monte Carlo (SMC)</span>**\n-   **<span style=\"color: #FFFF00;\">Particle MCMC (pMCMC)</span>**\n-   **<span style=\"color: #FFFF00;\">Approximate Bayesian Computation (ABC)</span>**\n\n**<span style=\"color: tomato;\">Today's Focus:</span>** Least Squares and MLE as foundations\n\n\n# Conceptual Foundations\n\n------------------------------------------------------------------------\n\n## What is Model Fitting?\n\n**<span style=\"color: #FFFF00;\">Definition:</span>** The process of finding parameter values that make a mathematical model's predictions as close as possible to observed data.\n\n------------------------------------------------------------------------\n\n## Mathematical Formulation\n\n$$\\hat{\\theta} = \\arg\\min_{\\theta} \\mathcal{L}(\\theta, \\mathbf{y})$$\n\nWhere:\n\n-   $\\theta$ = parameter vector (e.g., $\\beta, \\gamma$)\n-   $\\mathbf{y}$ = observed data\n-   $\\mathcal{L}$ = loss/objective function\n\n------------------------------------------------------------------------\n\n## The SIR Model as Our Example\n\n**Differential Equations:**\n\n$$\\frac{dS}{dt} = -\\beta SI$$ $$\\frac{dI}{dt} = \\beta SI - \\gamma I$$ $$\\frac{dR}{dt} = \\gamma I$$\n\n------------------------------------------------------------------------\n\n### Parameters to Estimate\n\n-   <span style=\"color: tomato;\">$\\beta$ = transmission rate</span>\n-   <span style=\"color: tomato;\">$\\gamma$ = recovery rate</span>\n\n------------------------------------------------------------------------\n\n## Key Quantities\n\n-   <span style=\"color: #FFFF00;\">$R_0 = \\frac{\\beta}{\\gamma}$ (basic reproduction number)</span>\n-   <span style=\"color: #FFFF00;\">Infectious period = $\\frac{1}{\\gamma}$</span>\n\n------------------------------------------------------------------------\n\n## Sources of Uncertainty\n\n### Model Uncertainty\n\n-   Wrong model structure\n-   Missing compartments or processes\n\n------------------------------------------------------------------------\n\n### Parameter Uncertainty\n\n-   True parameter values unknown\n-   Multiple parameter sets give similar fits\n\n------------------------------------------------------------------------\n\n### Observation Uncertainty\n\n-   Measurement error\n-   Reporting delays\n-   Underreporting\n\n------------------------------------------------------------------------\n\n### Process Uncertainty\n\n-   Stochasticity in disease transmission\n-   Environmental variability\n\n------------------------------------------------------------------------\n\n## The Fitting Challenge\n\n**<span style=\"color: tomato;\">Identifiability Problem:</span>** Multiple parameter combinations can produce similar model outputs\n\n**Example:**\n\n-   High $\\beta$, high $\\gamma$\n-   Low $\\beta$, low $\\gamma$\n\nBoth might give similar epidemic curves!\n\n**<span style=\"color: #FFFF00;\">Solution:</span>** Use additional information (e.g., known infectious period)\n\n\n# Least Squares Estimation\n\n------------------------------------------------------------------------\n\n## Least Squares: The Intuitive Approach\n\n**<span style=\"color: #FFFF00;\">Core Idea:</span>** Minimize the sum of squared differences between model predictions and observations\n\n------------------------------------------------------------------------\n\n**Mathematical Formulation:** $$\\text{SSE} = \\sum_{i=1}^{n} (y_i - f(t_i, \\theta))^2$$\n\nWhere:\n\n- $y_i$ = observed value at time $t_i$\n- $f(t_i, \\theta)$ = model prediction at time $t_i$\n- $\\theta$ = parameter vector\n\n------------------------------------------------------------------------\n\n### Why Squared Errors?\n\n**<span style=\"color: #FFFF00;\">Advantages:</span>**\n\n-   <span style=\"color: #FFFF00;\">Penalizes large errors more heavily</span>\n-   <span style=\"color: #FFFF00;\">Differentiable (smooth optimization)</span>\n-   <span style=\"color: #FFFF00;\">Mathematically tractable</span>\n-   <span style=\"color: #FFFF00;\">Gives maximum likelihood estimates when errors are normal distributed</span>\n\n------------------------------------------------------------------------\n\n### <span style=\"color: tomato;\">Disadvantages</span>\n\n-   <span style=\"color: tomato;\">Sensitive to outliers</span>\n-   <span style=\"color: tomato;\">Assumes constant variance</span>\n-   <span style=\"color: tomato;\">No probabilistic interpretation</span>\n\n------------------------------------------------------------------------\n\n## Least Squares Example: SIR Model\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/ls-example-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Implementing Least Squares\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define objective function\nsse_function <- function(params, data) {\n  beta <- params[1]\n  gamma <- params[2]\n  \n  # Simulate model\n  out <- ode(\n    y = init_conds,\n    times = data$time,\n    func = sir_model,\n    parms = c(beta = beta, gamma = gamma)\n  )\n  \n  # Calculate sum of squared errors\n  predicted <- out[, \"I\"] * 1000\n  sse <- sum((data$observed - predicted)^2)\n  \n  return(sse)\n}\n\n# Prepare data\ndata <- data.frame(time = times, observed = observed_cases)\n\n# Test different parameter values\ntest_params <- expand.grid(\n  beta = seq(0.1, 0.5, length.out = 10),\n  gamma = seq(0.05, 0.2, length.out = 10)\n)\n\n# Calculate SSE for each combination\nfor(i in 1:nrow(test_params)){\n  test_params$sse[i] <- sse_function(\n    c(\n      test_params[i, 1],\n      test_params[i, 2]\n    ),\n    data\n  )\n}\n\n# Find minimum\nbest_idx <- which.min(test_params$sse)\nbest_params <- test_params[best_idx, ]\ncat(\"True parameters: Beta =\", true_params[1], \", Gamma =\", true_params[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue parameters: Beta = 0.3 , Gamma = 0.1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Best fit parameters: Beta =\", round(best_params$beta, 3), \", Gamma =\", round(best_params$gamma, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBest fit parameters: Beta = 0.322 , Gamma = 0.1 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Strengths of Least Squares\n\n**Computational Advantages:**\n\n-   Fast and efficient\n-   Well-established algorithms\n-   Easy to implement\n-   Good for initial parameter estimates\n\n------------------------------------------------------------------------\n\n## Statistical Properties\n\n-   Unbiased estimates (under certain conditions)\n-   Minimum variance among linear unbiased estimators\n-   Maximum likelihood when errors are normal\n\n------------------------------------------------------------------------\n\n## Practical Benefits\n\n-   Intuitive interpretation\n-   Widely understood\n-   Good starting point for more complex methods\n\n------------------------------------------------------------------------\n\n## Limitations of Least Squares\n\n**Statistical Limitations:**\n\n-   Limited uncertainty quantification\n-   Assumes constant variance\n-   Sensitive to outliers\n-   No probabilistic framework\n\n------------------------------------------------------------------------\n\n## Practical Limitations\n\n**Practical Limitations:**\n\n-   Parameter identifiability issues\n-   No confidence intervals\n-   Difficult to compare models\n-   Assumes measurement error only\n\n------------------------------------------------------------------------\n\n## Example Problem\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show how different parameter combinations can give similar fits\nparam_combos <- data.frame(\n  beta = c(0.25, 0.35, 0.30),\n  gamma = c(0.08, 0.12, 0.10),\n  label = c(\"Low β, Low γ\", \"High β, High γ\", \"True\")\n)\n\n# Plot different fits\nggplot(plot_data, aes(x = time)) +\n  geom_point(aes(y = observed), color = \"red\", alpha = 0.7) +\n  geom_line(aes(y = true_model), color = \"blue\", linetype = 1) +\n  labs(x = \"Time (days)\", y = \"Number of Cases\",\n       title = \"Multiple Parameter Sets Can Give Similar Fits\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/ls-limitation-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Add lines for different parameter combinations\nout_test <- ode(\n    y = init_conds,\n    times = times,\n    func = sir_model,\n    parms = c(beta = param_combos$beta[1],\n              gamma = param_combos$gamma[1]\n    ))\n\nplot(times, out_test[, \"I\"] * 1000,\n     col = c(\"green\", \"orange\", \"purple\")[1],\n     lty = 2, lwd = 2\n)\n\nfor(i in 2:nrow(param_combos)) {\n  out_test <- ode(\n    y = init_conds,\n    times = times,\n    func = sir_model,\n    parms = c(beta = param_combos$beta[i],\n              gamma = param_combos$gamma[i]\n    ))\n  lines(times, out_test[, \"I\"] * 1000,\n        col = c(\"green\", \"orange\", \"purple\")[i],\n        lty = 2, lwd = 2\n  )\n}\n```\n\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/ls-limitation-2.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## R implementation practicals {background-color=\"#447099\" transition=\"fade-in\"}\n\n- Let's turn to the [tutorials](https://github.com/jamesmbaazam/mppr_intro_to_fitting_practicals)\n\n\n# Maximum Likelihood Estimation\n\n------------------------------------------------------------------------\n\n## Maximum Likelihood: The Probabilistic Approach\n\n**<span style=\"color: #FFFF00;\">Core Idea:</span>** Find parameter values that make the observed data most probable\n\n**Mathematical Formulation:** $$\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta) = \\arg\\max_{\\theta} \\prod_{i=1}^{n} f(y_i | \\theta)$$\n\nWhere: - $L(\\theta)$ = likelihood function - $f(y_i | \\theta)$ = probability density of observation $i$\n\n------------------------------------------------------------------------\n\n**In Practice:** Maximize log-likelihood $$\\hat{\\theta} = \\arg\\max_{\\theta} \\ell(\\theta) = \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(y_i | \\theta)$$\n\n------------------------------------------------------------------------\n\n## Why Maximum Likelihood?\n\n**<span style=\"color: #FFFF00;\">Theoretical Advantages:</span>**\n\n-   <span style=\"color: #FFFF00;\">Principled statistical framework</span>\n-   <span style=\"color: #FFFF00;\">Provides uncertainty quantification</span>\n-   <span style=\"color: #FFFF00;\">Enables model comparison (AIC, BIC)</span>\n-   <span style=\"color: #FFFF00;\">Asymptotically optimal properties</span>\n\n------------------------------------------------------------------------\n\n**<span style=\"color: tomato;\">Practical Benefits:</span>**\n\n-   <span style=\"color: tomato;\">Confidence intervals</span>\n-   <span style=\"color: tomato;\">Hypothesis testing</span>\n-   <span style=\"color: tomato;\">Model selection</span>\n-   <span style=\"color: tomato;\">Incorporates different error structures</span>\n\n------------------------------------------------------------------------\n\n## Choosing a Probability Distribution\n\n**For Count Data (Cases):**\n\n-   **Poisson**: $Y_i \\sim \\text{Poisson}(\\lambda_i)$\n-   **Negative Binomial**: $Y_i \\sim \\text{NB}(\\mu_i, \\phi)$\n\n------------------------------------------------------------------------\n\n## Choosing a Probability Distribution\n\n**For Continuous Data:**\n\n-   **Normal**: $Y_i \\sim N(\\mu_i, \\sigma^2)$\n-   **Log-normal**: $\\log Y_i \\sim N(\\log \\mu_i, \\sigma^2)$\n\n**For Our SIR Example:** We'll use Poisson since we're modeling case counts\n\n------------------------------------------------------------------------\n\n## MLE Implementation: Poisson Likelihood\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define negative log-likelihood function\nnll_function <- function(beta, gamma, data) {\n  # Simulate model\n  out <- ode(y = init_conds, times = data$time,\n             func = sir_model, parms = c(beta = beta, gamma = gamma))\n\n  # Model predictions (scaled to cases)\n  predicted <- out[,\"I\"] * 1000\n\n  # Poisson negative log-likelihood\n  nll <- -sum(dpois(data$observed, lambda = predicted, log = TRUE))\n\n  return(nll)\n}\n\n# Use optimization to find MLE\nlibrary(bbmle)\nfit_mle <- mle2(nll_function,\n                start = list(beta = 0.2, gamma = 0.1),\n                data = list(data = data),\n                method = \"L-BFGS-B\",\n                lower = c(0.01, 0.01),\n                upper = c(1.0, 0.5))\n\n# Extract results\nmle_params <- coef(fit_mle)\nmle_se <- sqrt(diag(vcov(fit_mle)))\n\ncat(\"MLE Results:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE Results:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Beta:\", round(mle_params[1], 3), \"±\", round(mle_se[1], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBeta: 0.291 ± 0.003 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Gamma:\", round(mle_params[2], 3), \"±\", round(mle_se[2], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGamma: 0.05 ± 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R0:\", round(mle_params[1]/mle_params[2], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR0: 5.79 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Uncertainty Quantification with MLE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Profile likelihood for uncertainty\nprof <- profile(fit_mle)\nplot(prof, absVal = TRUE, main = \"Profile Likelihood\")\n```\n\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/mle-uncertainty-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Confidence intervals\nconfint(fit_mle, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           2.5 %     97.5 %\nbeta  0.28620452 0.29599639\ngamma 0.04961407 0.05089814\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with true values\ncat(\"\\nComparison with True Values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComparison with True Values:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True Beta:\", true_params[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue Beta: 0.3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MLE Beta:\", round(mle_params[1], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE Beta: 0.291 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True Gamma:\", true_params[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue Gamma: 0.1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MLE Gamma:\", round(mle_params[2], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE Gamma: 0.05 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Model Comparison with MLE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit different models and compare\n# Model 1: SIR with Poisson\n# Model 2: SIR with Negative Binomial\n\n# Negative Binomial likelihood\nnll_nb <- function(beta, gamma, phi, data) {\n  out <- ode(y = init_conds, times = data$time,\n             func = sir_model, parms = c(beta = beta, gamma = gamma))\n\n  predicted <- out[,\"I\"] * 1000\n\n  # Negative Binomial negative log-likelihood\n  nll <- -sum(dnbinom(data$observed, mu = predicted, size = phi, log = TRUE))\n\n  return(nll)\n}\n\n# Fit NB model\nfit_nb <- mle2(nll_nb,\n               start = list(beta = 0.2, gamma = 0.1, phi = 10),\n               data = list(data = data),\n               method = \"L-BFGS-B\",\n               lower = c(0.01, 0.01, 0.1),\n               upper = c(1.0, 0.5, 100))\n\n# Compare models\ncat(\"Model Comparison:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel Comparison:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Poisson AIC:\", AIC(fit_mle), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoisson AIC: 19650.9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Negative Binomial AIC:\", AIC(fit_nb), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNegative Binomial AIC: 3189.442 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Delta AIC:\", AIC(fit_nb) - AIC(fit_mle), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDelta AIC: -16461.45 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Strengths of Maximum Likelihood\n\n**Statistical Rigor:**\n\n-   Principled probabilistic framework\n-   Asymptotic optimality properties\n-   Natural uncertainty quantification\n-   Enables formal hypothesis testing\n\n------------------------------------------------------------------------\n\n## Practical Benefits\n\n-   Confidence intervals and standard errors\n-   Model comparison via AIC/BIC\n-   Handles different error structures\n-   Extensible to complex models\n\n------------------------------------------------------------------------\n\n## Limitations of Maximum Likelihood\n\n**Computational Challenges:**\n\n-   More complex than least squares\n-   Requires optimization algorithms\n-   Can get stuck in local minima\n-   Sensitive to starting values\n\n------------------------------------------------------------------------\n\n## Statistical Assumptions\n\n-   Requires specification of error distribution\n-   Assumes model structure is correct\n-   Asymptotic properties may not hold\n-   Can be sensitive to outliers\n\n------------------------------------------------------------------------\n\n## Identifiability Issues\n\n-   Still suffers from parameter identifiability\n-   Profile likelihood can be computationally expensive\n-   May not converge for complex models\n\n------------------------------------------------------------------------\n\n## R implementation practicals {background-color=\"#447099\" transition=\"fade-in\"}\n\n- Let's turn to the [tutorials](https://github.com/jamesmbaazam/mppr_intro_to_fitting_practicals)\n\n\n# Comparison of Methods\n\n------------------------------------------------------------------------\n\n## When to Use Each Method\n\n**Use Least Squares When:**\n\n-   Quick exploratory analysis needed\n-   Getting initial parameter estimates\n-   Computational speed is critical\n-   Simple error structure assumed\n\n------------------------------------------------------------------------\n\n**Use Maximum Likelihood When:**\n\n-   Uncertainty quantification needed\n-   Comparing different models\n-   Formal statistical inference required\n-   Complex error structures present\n-   Publication-quality results needed\n\n\n# Advanced Methods\n\n------------------------------------------------------------------------\n\n## Beyond Least Squares and MLE\n\n[**Why We Need Advanced Methods:**]{style=\"color: tomato;\"}\n\n-   Parameter identifiability issues\n-   Complex error structures\n-   Model uncertainty\n-   Computational challenges\n-   Real-time fitting requirements\n\n------------------------------------------------------------------------\n\n## Advanced Approaches\n\n1.  **Bayesian Methods** (MCMC)\n2.  **Particle Filtering**\n3.  **Approximate Bayesian Computation (ABC)**\n4.  **Ensemble Methods**\n\n------------------------------------------------------------------------\n\n## Bayesian Methods: MCMC\n\n[**Core Idea:**]{style=\"color: #FFFF00;\"} Treat parameters as random variables with prior distributions\n\n**Bayes' Theorem:** $$P(\\theta | \\mathbf{y}) = \\frac{P(\\mathbf{y} | \\theta) P(\\theta)}{P(\\mathbf{y})}$$\n\n------------------------------------------------------------------------\n\n## Advantages\n\n-   Natural uncertainty quantification\n-   Incorporates prior knowledge\n-   Handles parameter identifiability\n-   Model comparison via Bayes factors\n\n------------------------------------------------------------------------\n\n## Example with Stan\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3-15|17-24|26-30|32-40|42-47|49-52\"}\n# Stan model for SIR fitting\n\"\n// SIR model in Stan (walkthrough)                                        \n// Functions block: derivative of [S, I, R]                               \nfunctions {\n  vector SIR(real t, vector y, array[] real theta) {                      \n    real S = y[1]; real I = y[2]; real R = y[3];\n    real beta = theta[1]; real gamma = theta[2];\n    vector[3] dydt;\n    dydt[1] = -beta * S * I;\n    dydt[2] =  beta * S * I - gamma * I;\n    dydt[3] =  gamma * I;\n    return dydt;\n  }\n}\n\n// Data block: obs counts & time grid                                     \ndata {\n  int<lower=1> n_obs;\n  int<lower=1> n_pop;\n  array[n_obs] int y;\n  real t0;\n  array[n_obs] real ts;\n}\n\n// Parameters: beta, gamma, S0                                             \nparameters {\n  array[2] real<lower=0> theta;     // {beta, gamma}\n  real<lower=0,upper=1> S0;         // initial susceptible fraction\n}\n\n// Transformed params: ODE solve + Poisson rate                            \ntransformed parameters {\n  vector[3] y_init;\n  array[n_obs] vector[3] y_hat;\n  array[n_obs] real lambda;\n  y_init[1] = S0; y_init[2] = 1 - S0; y_init[3] = 0;\n  y_hat = ode_rk45(SIR, y_init, t0, ts, theta);\n  for (i in 1:n_obs) lambda[i] = y_hat[i, 2] * n_pop;\n}\n\n// Model: priors + likelihood                                             \nmodel {\n  theta ~ lognormal(0, 1);\n  S0    ~ beta(1, 1);\n  y     ~ poisson(lambda);\n}\n\n// GQ: derived R0                                                          \ngenerated quantities {\n  real R_0 = theta[1] / theta[2];\n}\n\"\n```\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Particle Filtering\n\n[**Core Idea:**]{style=\"color: #FFFF00;\"} Sequential Monte Carlo method for state-space models\n\n**When to Use:**\n\n-   Real-time parameter estimation\n-   State estimation in stochastic models\n-   Handling of missing data\n-   Time-varying parameters\n\n------------------------------------------------------------------------\n\n## Advantages\n\n-   Handles stochasticity naturally\n-   Real-time updates\n-   No assumption of constant parameters\n-   Robust to model misspecification\n\n------------------------------------------------------------------------\n\n## Example Application\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Particle filter for SIR model\nlibrary(pomp)\n\n# Define SIR model with stochasticity\nsir_pomp <- pomp(\n  data = data.frame(time = covid_times, cases = covid_observed),\n  times = \"time\",\n  t0 = 0,\n  rprocess = euler.sim(\n    step.fun = \"sir_step\",\n    delta.t = 0.1\n  ),\n  rmeasure = \"cases_measure\",\n  dmeasure = \"cases_dmeasure\",\n  initializer = \"sir_init\",\n  paramnames = c(\"beta\", \"gamma\", \"sigma\"),\n  statenames = c(\"S\", \"I\", \"R\")\n)\n\n# Run particle filter\npf <- pfilter(sir_pomp, Np = 1000, params = c(beta = 0.3, gamma = 0.1, sigma = 1))\n```\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Approximate Bayesian Computation (ABC)\n\n[**Core Idea:**]{style=\"color: #FFFF00;\"} Approximate posterior without likelihood evaluation\n\n**When to Use:**\n\n-   Complex likelihoods\n-   Intractable models\n-   High-dimensional parameter spaces\n-   Model comparison\n\n------------------------------------------------------------------------\n\n**Algorithm:**\n\n1.  Sample parameters from prior\n2.  Simulate data from model\n3.  Compare simulated to observed data\n4.  Accept if distance \\< threshold\n\n------------------------------------------------------------------------\n\n**Advantages:**\n\n-   No likelihood required\n-   Handles complex models\n-   Model comparison\n-   Intuitive approach\n\n------------------------------------------------------------------------\n\n## Ensemble Methods\n\n[**Core Idea:**]{style=\"color: #FFFF00;\"} Combine multiple models or methods\n\n**Types:**\n\n-   **Model Ensembles**: Average predictions from different models\n-   **Method Ensembles**: Combine LS, MLE, MCMC results\n-   **Bootstrap Ensembles**: Multiple fits with resampled data\n\n------------------------------------------------------------------------\n\n## Advantages:\n\n-   Reduces overfitting\n-   Quantifies model uncertainty\n-   More robust predictions\n-   Handles model selection uncertainty\n\n\n# Practical Considerations\n\n------------------------------------------------------------------------\n\n## Common Fitting Problems\n\n**Convergence Issues:**\n\n-   Poor starting values\n-   Flat likelihood surfaces\n-   Numerical instabilities\n-   Parameter bounds\n\n------------------------------------------------------------------------\n\n**Identifiability Problems:**\n\n-   Multiple solutions\n-   Correlated parameters\n-   Insufficient data\n-   Model overparameterization\n\n------------------------------------------------------------------------\n\n**Solutions:**\n\n-   Multiple starting points\n-   Profile likelihood\n-   Data augmentation\n\n------------------------------------------------------------------------\n\n## Troubleshooting Guide\n\n**If Optimization Fails:**\n\n1.  Check starting values\n2.  Verify parameter bounds\n3.  Examine objective function\n4.  Try different algorithms\n5.  Simplify the model\n\n------------------------------------------------------------------------\n\n**If Parameters Are Unidentifiable:**\n\n1.  Fix some parameters\n2.  Use additional data\n3.  Add regularization\n4.  Consider model reduction\n5.  Use prior information\n\n------------------------------------------------------------------------\n\n**If Results Are Unrealistic:**\n\n1.  Check model assumptions\n2.  Verify data quality\n3.  Examine residuals\n4.  Test sensitivity\n5.  Consider alternative models\n\n------------------------------------------------------------------------\n\n## Best Practices\n\n**Before Fitting:**\n\n-   Understand your data\n-   Check model assumptions\n-   Set realistic parameter bounds\n-   Prepare multiple starting values\n\n------------------------------------------------------------------------\n\n**During Fitting:**\n\n-   Monitor convergence\n-   Check for local minima\n-   Validate results\n-   Document everything\n\n------------------------------------------------------------------------\n\n**After Fitting:**\n\n-   Assess goodness of fit\n-   Quantify uncertainty\n-   Test sensitivity\n-   Validate predictions\n\n------------------------------------------------------------------------\n\n## Software Recommendations\n\n**R Packages:**\n\n-   `bbmle`: Maximum likelihood\n-   `rstan`: Bayesian inference\n-   `pomp`: Particle filtering\n\n------------------------------------------------------------------------\n\n**Specialized Software:**\n\n-   `Stan`: Probabilistic programming\n-   `JAGS`: Bayesian analysis\n-   `PyMC`: Bayesian inference\n\n\n\n# Conclusions\n\n------------------------------------------------------------------------\n\n## Key Takeaways\n\n**Least Squares:**\n\n-   Fast and intuitive\n-   Good for exploration\n-   Limited uncertainty quantification\n-   Sensitive to assumptions\n\n------------------------------------------------------------------------\n\n**Maximum Likelihood:**\n\n-   Principled statistical framework\n-   Natural uncertainty quantification\n-   Enables model comparison\n-   More computationally intensive\n\n------------------------------------------------------------------------\n\n**Advanced Methods:**\n\n-   Handle complex scenarios\n-   Provide robust uncertainty\n-   Require more expertise\n-   Often computationally expensive\n\n------------------------------------------------------------------------\n\n## Choosing the Right Method\n\n**For Quick Exploration:** Least Squares\n\n**For Publication:** Maximum Likelihood\n\n**For Complex Models:** Bayesian Methods\n\n**For Real-time:** Particle Filtering\n\n**For Model Comparison:** ABC or MCMC\n\n[**General Principle:**]{style=\"color: tomato;\"} Start simple, add complexity as needed\n\n------------------------------------------------------------------------\n\n## Final Thoughts\n\n[**Model fitting is both art and science:**]{style=\"color: tomato;\"}\n\n-   Requires domain expertise\n-   Demands statistical rigor\n-   Benefits from computational tools\n-   Needs careful validation\n\n------------------------------------------------------------------------\n\n[**The goal is not just to fit models, but to:**]{style=\"color: #FFFF00;\"}\n\n-   Understand disease dynamics\n-   Make reliable predictions\n-   Inform policy decisions\n-   Advance scientific knowledge\n\n------------------------------------------------------------------------\n\n::: {.fragment style=\"font-size: 450%\"}\nAny Questions?\n:::\n\n## Resources\n\n### Full courses (free)\n\n-   Model fitting and inference for infectious disease dynamics by Sebastian Funk, Anton Camacho, Helen Johnson, Amanda Minter, Kathleen O'Reilly and Nicholas Davies. [Link](https://sbfnk.github.io/mfiidd/index.html)\n\n------------------------------------------------------------------------\n\n### Core concepts\n\n-   [Introduction to the Concept of Likelihood and Its Applications](https://journals.sagepub.com/doi/10.1177/2515245917744314)\n-   *Key considerations for model fitting and calibration*: [Choices and trade-offs in inference with infectious disease models](https://www.sciencedirect.com/science/article/pii/S1755436519300441?via%3Dihub)\n-   *A compilation of model fitting tutorials*: [Tooling-up for infectious disease transmission modelling](https://www.sciencedirect.com/science/article/pii/S1755436520300220)\n\n------------------------------------------------------------------------\n\n### Least squares\n\n-   *Key tutorial on the least squares method*: [Fitting dynamic models to epidemic outbreaks with quantified uncertainty: A primer for parameter uncertainty, identifiability, and forecasts](https://www.sciencedirect.com/science/article/pii/S2468042717300234)\n-   [Fitting Epidemic Models to Data by James Holland Jones](https://web.stanford.edu/class/earthsys214/notes/fit.html)\n\n------------------------------------------------------------------------\n\n### MLE\n\n-   [Fitting Epidemic Models to Data by James Holland Jones](https://web.stanford.edu/class/earthsys214/notes/fit.html)\n-   *R tutorial on MLE*: [Estimating model parameters by maximum likelihood](https://daphnia.ecology.uga.edu/drakelab/wp-content/uploads/2014/07/likelihood.pdf)\n\n------------------------------------------------------------------------\n\n### MCMC\n\n-   [Markov Chain Monte Carlo: an introduction for epidemiologists](10.1093/ije/dyt043)\n-   [A simple introduction to Markov Chain Monte–Carlo sampling](10.3758/s13423-016-1015-8)\n\n------------------------------------------------------------------------\n\n### pMCMC\n\n-   [Introduction to particle Markov-chain Monte Carlo for disease dynamics modellers](https://www.sciencedirect.com/science/article/pii/S1755436519300301)\n\n------------------------------------------------------------------------\n\n### ABC\n\n-   [Approximate Bayesian Computation for infectious disease modelling](%5B10.1016/j.epidem.2019.100368%5D(https://doi.org/10.1016/j.epidem.2019.100368))\n\n------------------------------------------------------------------------\n\n### Others\n\n-   [Bayesian workflow for disease transmission modeling in Stan](https://mc-stan.org/learn-stan/case-studies/boarding_school_case_study.html)\n-   [POMP](https://kingaa.github.io/pomp/vignettes/getting_started.html)\n-   [Odin and Monty](https://mrc-ide.github.io/odin-monty/)\n\n## References\n\n-   Anderson, R.M. and May, R.M. (1991). *Infectious Diseases of Humans: Dynamics and Control*. Oxford University Press.\n-   Keeling, M.J. and Rohani, P. (2008). *Modeling Infectious Diseases in Humans and Animals*. Princeton University Press.\n-   Bolker, B. (2008). *Ecological Models and Data in R*. Princeton University Press.\n-   King, A.A. et al. (2016). \"Statistical inference for partially observed Markov processes via the R package pomp.\" *Journal of Statistical Software*.\n-   Carpenter, B. et al. (2017). \"Stan: A probabilistic programming language.\" *Journal of Statistical Software*.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}