{
  "hash": "92516d91c314449732891f9384c951e2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Model Fitting and Calibration\"\nsubtitle: \"Compartmental Models in Epidemiology\"\nauthor:\n  - name: \"James Mba Azam, PhD\"\n    orcid: 0000-0001-5782-7330\n    email: james.azam@lshtm.ac.uk\n    affiliation:\n      - name: London School of Hygiene and Tropical Medicine, UK\n        city: London, United Kingdom\ndate: \"last-modified\"\nlicense: \"CC BY\"\ncopyright:\n  holder: James Mba Azam\n  year: 2025\nexecute:\n  echo: true\n  cache: true\n  message: false\n  warning: false\n  freeze: auto\nformat:\n  revealjs:\n    theme: [solarized]\n    slide-number: true\n    scrollable: true\n    chalkboard: true\n    toc: true\n    toc-depth: 1\n    transition: fade\n    lightbox: true\n    progress: true\n    code-copy: true\n# bibliography: references.bib\nlink-citations: true\nengine: knitr\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n## Learning Objectives\n\nBy the end of this lecture, you will be able to:\n\n-   Understand the fundamental concepts of model fitting and calibration\n-   Apply least squares estimation to compartmental models\n-   Implement maximum likelihood estimation for epidemic models\n-   Compare the strengths and weaknesses of different fitting methods\n-   Recognize when to use advanced methods like MCMC and particle filtering\n-   Troubleshoot common fitting problems\n\n# Part I: Introduction & Motivation\n\n------------------------------------------------------------------------\n\n## Why Model Fitting Matters\n\n### The Challenge\n\n**The Challenge:**\n\n-   We have mathematical models (e.g., SIR, SEIR)\n-   We have real-world data (case counts, hospitalizations)\n-   **<span style=\"color: tomato;\">How do we connect them?</span>**\n\n------------------------------------------------------------------------\n\n### The Goal\n\n-   <span style=\"color: blue;\">Find parameter values that make our model predictions match observed data</span>\n-   <span style=\"color: blue;\">Quantify uncertainty in our estimates</span>\n-   <span style=\"color: blue;\">Make reliable predictions and policy recommendations</span>\n\n------------------------------------------------------------------------\n\n## Example: Outbreak data\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/covid-example-1.png){width=960}\n:::\n:::\n\n\n\n**<span style=\"color: tomato;\">Question:</span>** How do we estimate $\\beta$ and $\\gamma$ from this noisy data?\n\n## Types of Fitting Methods\n\n### Deterministic Methods\n\n-   **<span style=\"color: tomato;\">Least Squares</span>**\n-   **<span style=\"color: tomato;\">Maximum Likelihood Estimation (MLE)</span>**\n\n------------------------------------------------------------------------\n\n### Stochastic Methods\n\n-   **<span style=\"color: blue;\">Markov Chain Monte Carlo (MCMC)</span>**\n-   **<span style=\"color: blue;\">Sequential Monte Carlo (SMC)</span>**\n-   **<span style=\"color: blue;\">Particle MCMC (pMCMC)</span>**\n-   **<span style=\"color: blue;\">Approximate Bayesian Computation (ABC)</span>**\n\n**<span style=\"color: tomato;\">Today's Focus:</span>** Least Squares and MLE as foundations\n\n# Part II: Conceptual Foundations\n\n------------------------------------------------------------------------\n\n## What is Model Fitting?\n\n**<span style=\"color: blue;\">Definition:</span>** The process of finding parameter values that make a mathematical model's predictions as close as possible to observed data.\n\n------------------------------------------------------------------------\n\n## Mathematical Formulation\n\n$$\\hat{\\theta} = \\arg\\min_{\\theta} \\mathcal{L}(\\theta, \\mathbf{y})$$\n\nWhere:\n\n-   $\\theta$ = parameter vector (e.g., $\\beta, \\gamma$)\n-   $\\mathbf{y}$ = observed data\n-   $\\mathcal{L}$ = loss/objective function\n\n------------------------------------------------------------------------\n\n## The SIR Model as Our Example\n\n**Differential Equations:**\n\n$$\\frac{dS}{dt} = -\\beta SI$$ $$\\frac{dI}{dt} = \\beta SI - \\gamma I$$ $$\\frac{dR}{dt} = \\gamma I$$\n\n------------------------------------------------------------------------\n\n### Parameters to Estimate\n\n-   <span style=\"color: tomato;\">$\\beta$ = transmission rate</span>\n-   <span style=\"color: tomato;\">$\\gamma$ = recovery rate</span>\n\n------------------------------------------------------------------------\n\n## Key Quantities\n\n-   <span style=\"color: blue;\">$R_0 = \\frac{\\beta}{\\gamma}$ (basic reproduction number)</span>\n-   <span style=\"color: blue;\">Infectious period = $\\frac{1}{\\gamma}$</span>\n\n------------------------------------------------------------------------\n\n## Sources of Uncertainty\n\n### Model Uncertainty\n\n-   Wrong model structure\n-   Missing compartments or processes\n\n------------------------------------------------------------------------\n\n### Parameter Uncertainty\n\n-   True parameter values unknown\n-   Multiple parameter sets give similar fits\n\n------------------------------------------------------------------------\n\n### Observation Uncertainty\n\n-   Measurement error\n-   Reporting delays\n-   Underreporting\n\n------------------------------------------------------------------------\n\n### Process Uncertainty\n\n-   Stochasticity in disease transmission\n-   Environmental variability\n\n------------------------------------------------------------------------\n\n## The Fitting Challenge\n\n**<span style=\"color: tomato;\">Identifiability Problem:</span>** Multiple parameter combinations can produce similar model outputs\n\n**Example:**\n\n-   High $\\beta$, high $\\gamma$\n-   Low $\\beta$, low $\\gamma$\n\nBoth might give similar epidemic curves!\n\n**<span style=\"color: blue;\">Solution:</span>** Use additional information (e.g., known infectious period)\n\n# Part III: Least Squares Estimation\n\n------------------------------------------------------------------------\n\n## Least Squares: The Intuitive Approach\n\n**<span style=\"color: blue;\">Core Idea:</span>** Minimize the sum of squared differences between model predictions and observations\n\n------------------------------------------------------------------------\n\n**Mathematical Formulation:** $$\\text{SSE} = \\sum_{i=1}^{n} (y_i - f(t_i, \\theta))^2$$\n\nWhere:\n\n- $y_i$ = observed value at time $t_i$\n- $f(t_i, \\theta)$ = model prediction at time $t_i$\n- $\\theta$ = parameter vector\n\n------------------------------------------------------------------------\n\n### Why Squared Errors?\n\n**<span style=\"color: blue;\">Advantages:</span>**\n\n-   <span style=\"color: blue;\">Penalizes large errors more heavily</span>\n-   <span style=\"color: blue;\">Differentiable (smooth optimization)</span>\n-   <span style=\"color: blue;\">Mathematically tractable</span>\n-   <span style=\"color: blue;\">Gives maximum likelihood estimates when errors are normal distributed</span>\n\n------------------------------------------------------------------------\n\n### <span style=\"color: tomato;\">Disadvantages</span>\n\n-   <span style=\"color: tomato;\">Sensitive to outliers</span>\n-   <span style=\"color: tomato;\">Assumes constant variance</span>\n-   <span style=\"color: tomato;\">No probabilistic interpretation</span>\n\n------------------------------------------------------------------------\n\n## Least Squares Example: SIR Model\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/ls-example-1.png){width=960}\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Implementing Least Squares\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define objective function\nsse_function <- function(params, data) {\n  beta <- params[1]\n  gamma <- params[2]\n  \n  # Simulate model\n  out <- ode(\n    y = init_conds,\n    times = data$time,\n    func = sir_model,\n    parms = c(beta = beta, gamma = gamma)\n  )\n  \n  # Calculate sum of squared errors\n  predicted <- out[, \"I\"] * 1000\n  sse <- sum((data$observed - predicted)^2)\n  \n  return(sse)\n}\n\n# Prepare data\ndata <- data.frame(time = times, observed = observed_cases)\n\n# Test different parameter values\ntest_params <- expand.grid(\n  beta = seq(0.1, 0.5, length.out = 10),\n  gamma = seq(0.05, 0.2, length.out = 10)\n)\n\n# Calculate SSE for each combination\nfor(i in 1:nrow(test_params)){\n  test_params$sse[i] <- sse_function(\n    c(\n      test_params[i, 1],\n      test_params[i, 2]\n    ),\n    data\n  )\n}\n\n# Find minimum\nbest_idx <- which.min(test_params$sse)\nbest_params <- test_params[best_idx, ]\ncat(\"True parameters: Beta =\", true_params[1], \", Gamma =\", true_params[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue parameters: Beta = 0.3 , Gamma = 0.1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Best fit parameters: Beta =\", round(best_params$beta, 3), \", Gamma =\", round(best_params$gamma, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBest fit parameters: Beta = 0.322 , Gamma = 0.1 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Strengths of Least Squares\n\n**Computational Advantages:**\n\n-   Fast and efficient\n-   Well-established algorithms\n-   Easy to implement\n-   Good for initial parameter estimates\n\n------------------------------------------------------------------------\n\n## Statistical Properties\n\n-   Unbiased estimates (under certain conditions)\n-   Minimum variance among linear unbiased estimators\n-   Maximum likelihood when errors are normal\n\n------------------------------------------------------------------------\n\n## Practical Benefits\n\n-   Intuitive interpretation\n-   Widely understood\n-   Good starting point for more complex methods\n\n------------------------------------------------------------------------\n\n## Limitations of Least Squares\n\n**Statistical Limitations:**\n\n-   Limited uncertainty quantification\n-   Assumes constant variance\n-   Sensitive to outliers\n-   No probabilistic framework\n\n------------------------------------------------------------------------\n\n## Practical Limitations\n\n**Practical Limitations:**\n\n-   Parameter identifiability issues\n-   No confidence intervals\n-   Difficult to compare models\n-   Assumes measurement error only\n\n------------------------------------------------------------------------\n\n## Example Problem\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show how different parameter combinations can give similar fits\nparam_combos <- data.frame(\n  beta = c(0.25, 0.35, 0.30),\n  gamma = c(0.08, 0.12, 0.10),\n  label = c(\"Low β, Low γ\", \"High β, High γ\", \"True\")\n)\n\n# Plot different fits\nggplot(plot_data, aes(x = time)) +\n  geom_point(aes(y = observed), color = \"red\", alpha = 0.7) +\n  geom_line(aes(y = true_model), color = \"blue\", linetype = 1) +\n  labs(x = \"Time (days)\", y = \"Number of Cases\",\n       title = \"Multiple Parameter Sets Can Give Similar Fits\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/ls-limitation-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Add lines for different parameter combinations\nout_test <- ode(\n    y = init_conds,\n    times = times,\n    func = sir_model,\n    parms = c(beta = param_combos$beta[1],\n              gamma = param_combos$gamma[1]\n    ))\n\nplot(times, out_test[, \"I\"] * 1000,\n     col = c(\"green\", \"orange\", \"purple\")[1],\n     lty = 2, lwd = 2\n)\n\nfor(i in 2:nrow(param_combos)) {\n  out_test <- ode(\n    y = init_conds,\n    times = times,\n    func = sir_model,\n    parms = c(beta = param_combos$beta[i],\n              gamma = param_combos$gamma[i]\n    ))\n  lines(times, out_test[, \"I\"] * 1000,\n        col = c(\"green\", \"orange\", \"purple\")[i],\n        lty = 2, lwd = 2\n  )\n}\n```\n\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/ls-limitation-2.png){width=960}\n:::\n:::\n\n\n\n# Part IV: Maximum Likelihood Estimation\n\n------------------------------------------------------------------------\n\n## Maximum Likelihood: The Probabilistic Approach\n\n**<span style=\"color: blue;\">Core Idea:</span>** Find parameter values that make the observed data most probable\n\n**Mathematical Formulation:** $$\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta) = \\arg\\max_{\\theta} \\prod_{i=1}^{n} f(y_i | \\theta)$$\n\nWhere: - $L(\\theta)$ = likelihood function - $f(y_i | \\theta)$ = probability density of observation $i$\n\n------------------------------------------------------------------------\n\n**In Practice:** Maximize log-likelihood $$\\hat{\\theta} = \\arg\\max_{\\theta} \\ell(\\theta) = \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(y_i | \\theta)$$\n\n------------------------------------------------------------------------\n\n## Why Maximum Likelihood?\n\n**<span style=\"color: blue;\">Theoretical Advantages:</span>**\n\n-   <span style=\"color: blue;\">Principled statistical framework</span>\n-   <span style=\"color: blue;\">Provides uncertainty quantification</span>\n-   <span style=\"color: blue;\">Enables model comparison (AIC, BIC)</span>\n-   <span style=\"color: blue;\">Asymptotically optimal properties</span>\n\n------------------------------------------------------------------------\n\n**<span style=\"color: tomato;\">Practical Benefits:</span>**\n\n-   <span style=\"color: tomato;\">Confidence intervals</span>\n-   <span style=\"color: tomato;\">Hypothesis testing</span>\n-   <span style=\"color: tomato;\">Model selection</span>\n-   <span style=\"color: tomato;\">Incorporates different error structures</span>\n\n------------------------------------------------------------------------\n\n## Choosing a Probability Distribution\n\n**For Count Data (Cases):**\n\n-   **Poisson**: $Y_i \\sim \\text{Poisson}(\\lambda_i)$\n-   **Negative Binomial**: $Y_i \\sim \\text{NB}(\\mu_i, \\phi)$\n\n------------------------------------------------------------------------\n\n## Choosing a Probability Distribution\n\n**For Continuous Data:**\n\n-   **Normal**: $Y_i \\sim N(\\mu_i, \\sigma^2)$\n-   **Log-normal**: $\\log Y_i \\sim N(\\log \\mu_i, \\sigma^2)$\n\n**For Our SIR Example:** We'll use Poisson since we're modeling case counts\n\n------------------------------------------------------------------------\n\n## MLE Implementation: Poisson Likelihood\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define negative log-likelihood function\nnll_function <- function(beta, gamma, data) {\n  # Simulate model\n  out <- ode(y = init_conds, times = data$time,\n             func = sir_model, parms = c(beta = beta, gamma = gamma))\n\n  # Model predictions (scaled to cases)\n  predicted <- out[,\"I\"] * 1000\n\n  # Poisson negative log-likelihood\n  nll <- -sum(dpois(data$observed, lambda = predicted, log = TRUE))\n\n  return(nll)\n}\n\n# Use optimization to find MLE\nlibrary(bbmle)\nfit_mle <- mle2(nll_function,\n                start = list(beta = 0.2, gamma = 0.1),\n                data = list(data = data),\n                method = \"L-BFGS-B\",\n                lower = c(0.01, 0.01),\n                upper = c(1.0, 0.5))\n\n# Extract results\nmle_params <- coef(fit_mle)\nmle_se <- sqrt(diag(vcov(fit_mle)))\n\ncat(\"MLE Results:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE Results:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Beta:\", round(mle_params[1], 3), \"±\", round(mle_se[1], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBeta: 0.291 ± 0.003 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Gamma:\", round(mle_params[2], 3), \"±\", round(mle_se[2], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGamma: 0.05 ± 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R0:\", round(mle_params[1]/mle_params[2], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR0: 5.79 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Uncertainty Quantification with MLE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Profile likelihood for uncertainty\nprof <- profile(fit_mle)\nplot(prof, absVal = TRUE, main = \"Profile Likelihood\")\n```\n\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/mle-uncertainty-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Confidence intervals\nconfint(fit_mle, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           2.5 %     97.5 %\nbeta  0.28620452 0.29599639\ngamma 0.04961407 0.05089814\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with true values\ncat(\"\\nComparison with True Values:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComparison with True Values:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True Beta:\", true_params[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue Beta: 0.3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MLE Beta:\", round(mle_params[1], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE Beta: 0.291 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True Gamma:\", true_params[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue Gamma: 0.1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MLE Gamma:\", round(mle_params[2], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMLE Gamma: 0.05 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Model Comparison with MLE\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit different models and compare\n# Model 1: SIR with Poisson\n# Model 2: SIR with Negative Binomial\n\n# Negative Binomial likelihood\nnll_nb <- function(beta, gamma, phi, data) {\n  out <- ode(y = init_conds, times = data$time,\n             func = sir_model, parms = c(beta = beta, gamma = gamma))\n\n  predicted <- out[,\"I\"] * 1000\n\n  # Negative Binomial negative log-likelihood\n  nll <- -sum(dnbinom(data$observed, mu = predicted, size = phi, log = TRUE))\n\n  return(nll)\n}\n\n# Fit NB model\nfit_nb <- mle2(nll_nb,\n               start = list(beta = 0.2, gamma = 0.1, phi = 10),\n               data = list(data = data),\n               method = \"L-BFGS-B\",\n               lower = c(0.01, 0.01, 0.1),\n               upper = c(1.0, 0.5, 100))\n\n# Compare models\ncat(\"Model Comparison:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel Comparison:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Poisson AIC:\", AIC(fit_mle), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoisson AIC: 19650.9 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Negative Binomial AIC:\", AIC(fit_nb), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNegative Binomial AIC: 3189.442 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Delta AIC:\", AIC(fit_nb) - AIC(fit_mle), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDelta AIC: -16461.45 \n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Strengths of Maximum Likelihood\n\n**Statistical Rigor:**\n\n-   Principled probabilistic framework\n-   Asymptotic optimality properties\n-   Natural uncertainty quantification\n-   Enables formal hypothesis testing\n\n------------------------------------------------------------------------\n\n## Practical Benefits\n\n-   Confidence intervals and standard errors\n-   Model comparison via AIC/BIC\n-   Handles different error structures\n-   Extensible to complex models\n\n------------------------------------------------------------------------\n\n## Limitations of Maximum Likelihood\n\n**Computational Challenges:**\n\n-   More complex than least squares\n-   Requires optimization algorithms\n-   Can get stuck in local minima\n-   Sensitive to starting values\n\n------------------------------------------------------------------------\n\n## Statistical Assumptions\n\n-   Requires specification of error distribution\n-   Assumes model structure is correct\n-   Asymptotic properties may not hold\n-   Can be sensitive to outliers\n\n------------------------------------------------------------------------\n\n## Identifiability Issues\n\n-   Still suffers from parameter identifiability\n-   Profile likelihood can be computationally expensive\n-   May not converge for complex models\n\n# Part V: Comparison of Methods\n\n------------------------------------------------------------------------\n\n## When to Use Each Method\n\n**Use Least Squares When:**\n\n-   Quick exploratory analysis needed\n-   Getting initial parameter estimates\n-   Computational speed is critical\n-   Simple error structure assumed\n\n------------------------------------------------------------------------\n\n**Use Maximum Likelihood When:**\n\n-   Uncertainty quantification needed\n-   Comparing different models\n-   Formal statistical inference required\n-   Complex error structures present\n-   Publication-quality results needed\n\n<!-- ------------------------------------------------------------------------ -->\n\n<!-- ## Practical Example: COVID-19 Fitting -->\n\n<!-- ```{r covid-comparison, echo=TRUE, fig.width=12, fig.height=8} -->\n<!-- # Simulate more realistic COVID-19 data -->\n<!-- set.seed(123) -->\n<!-- covid_times <- 1:100 -->\n<!-- covid_true_beta <- 0.25 -->\n<!-- covid_true_gamma <- 0.1 -->\n<!-- covid_true_R0 <- covid_true_beta / covid_true_gamma -->\n\n<!-- # Simulate with time-varying transmission -->\n<!-- beta_t <- covid_true_beta * (1 + 0.3 * sin(covid_times * 0.1)) -->\n<!-- out_covid <- ode(y = c(S = 0.99, I = 0.01, R = 0), -->\n<!--                  times = covid_times, -->\n<!--                  func = sir_model, -->\n<!--                  parms = c(beta = covid_true_beta, gamma = covid_true_gamma)) -->\n\n<!-- # Add realistic noise -->\n<!-- covid_observed <- rnbinom(length(covid_times), -->\n<!--                          mu = out_covid[,\"I\"] * 10000, -->\n<!--                          size = 5) -->\n\n<!-- # Fit both methods -->\n<!-- covid_data <- data.frame(time = covid_times, observed = covid_observed) -->\n\n<!-- # Least Squares -->\n<!-- ls_fit <- optim(c(0.02, 0.5), sse_function, data = covid_data) -->\n<!-- ls_params <- ls_fit$par -->\n\n<!-- # Maximum Likelihood -->\n<!-- mle_fit <- mle2(nll_function, -->\n<!--                 start = list(beta = 0.02, gamma = 0.5), -->\n<!--                 data = list(data = covid_data)) -->\n\n<!-- mle_params <- coef(mle_fit) -->\n<!-- mle_se <- sqrt(diag(vcov(mle_fit))) -->\n\n<!-- # Compare results -->\n<!-- comparison_df <- data.frame( -->\n<!--   Method = c(\"True\", \"Least Squares\", \"Maximum Likelihood\"), -->\n<!--   Beta = c(covid_true_beta, ls_params[1], mle_params[1]), -->\n<!--   Gamma = c(covid_true_gamma, ls_params[2], mle_params[2]), -->\n<!--   R0 = c(covid_true_R0, ls_params[1]/ls_params[2], mle_params[1]/mle_params[2]) -->\n<!-- ) -->\n\n<!-- print(comparison_df) -->\n<!-- ``` -->\n\n<!-- ------------------------------------------------------------------------ -->\n\n<!-- ## Visual Comparison of Fits -->\n\n<!-- ```{r visual-comparison, echo=TRUE, fig.width=12, fig.height=8} -->\n<!-- # Generate predictions from both methods -->\n<!-- ls_pred <- ode(y = c(S = 0.99, I = 0.01, R = 0), -->\n<!--                times = covid_times, -->\n<!--                func = sir_model, -->\n<!--                parms = c(beta = ls_params[1], gamma = ls_params[2]) -->\n<!--                ) -->\n\n<!-- mle_pred <- ode(y = c(S = 0.99, I = 0.01, R = 0), -->\n<!--                 times = covid_times, -->\n<!--                 func = sir_model, -->\n<!--                 parms = c(beta = unname(mle_params[1]), gamma = unname(mle_params[2])) -->\n<!--                 ) -->\n\n<!-- # Create comparison plot -->\n<!-- plot_data <- data.frame( -->\n<!--   time = covid_times, -->\n<!--   observed = covid_observed, -->\n<!--   true_model = out_covid[,\"I\"] * 10000, -->\n<!--   ls_fit = ls_pred[,\"I\"] * 10000, -->\n<!--   mle_fit = mle_pred[,\"I\"] * 10000 -->\n<!-- ) -->\n\n<!-- library(ggplot2) -->\n<!-- ggplot(plot_data, aes(x = time)) + -->\n<!--   geom_point(aes(y = observed), color = \"red\", alpha = 0.6, size = 1) + -->\n<!--   geom_line(aes(y = true_model), color = \"black\", linewidth = 1.5) + -->\n<!--   geom_line(aes(y = ls_fit), color = \"blue\", linetype = 1, linetype = \"dashed\") + -->\n<!--   geom_line(aes(y = mle_fit), color = \"green\", linetype = 1, linetype = \"dotted\") + -->\n<!--   labs(x = \"Time (days)\", y = \"Daily Cases\", -->\n<!--        title = \"Comparison of Fitting Methods\") + -->\n<!--   scale_color_manual(values = c(\"red\", \"black\", \"blue\", \"green\")) + -->\n<!--   theme_minimal() + -->\n<!--   theme(legend.position = \"bottom\") -->\n<!-- ``` -->\n\n# Part VI: Advanced Methods\n\n------------------------------------------------------------------------\n\n## Beyond Least Squares and MLE\n\n**<span style=\"color: tomato;\">Why We Need Advanced Methods:</span>**\n\n-   Parameter identifiability issues\n-   Complex error structures\n-   Model uncertainty\n-   Computational challenges\n-   Real-time fitting requirements\n\n------------------------------------------------------------------------\n\n## Advanced Approaches\n\n1.  **Bayesian Methods** (MCMC)\n2.  **Particle Filtering**\n3.  **Approximate Bayesian Computation (ABC)**\n4.  **Ensemble Methods**\n\n------------------------------------------------------------------------\n\n## Bayesian Methods: MCMC\n\n**<span style=\"color: blue;\">Core Idea:</span>** Treat parameters as random variables with prior distributions\n\n**Bayes' Theorem:** $$P(\\theta | \\mathbf{y}) = \\frac{P(\\mathbf{y} | \\theta) P(\\theta)}{P(\\mathbf{y})}$$\n\n------------------------------------------------------------------------\n\n## Advantages\n\n-   Natural uncertainty quantification\n-   Incorporates prior knowledge\n-   Handles parameter identifiability\n-   Model comparison via Bayes factors\n\n------------------------------------------------------------------------\n\n## Example with Stan\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Stan model for SIR fitting\nstan_code <- \"\ndata {\n  int<lower=0> N;\n  vector[N] time;\n  int<lower=0> cases[N];\n  real<lower=0> S0;\n  real<lower=0> I0;\n  real<lower=0> R0;\n}\n\nparameters {\n  real<lower=0> beta;\n  real<lower=0> gamma;\n  real<lower=0> sigma;\n}\n\nmodel {\n  // Priors\n  beta ~ normal(0.3, 0.1);\n  gamma ~ normal(0.1, 0.05);\n  sigma ~ exponential(1);\n\n  // Likelihood\n  for(i in 1:N) {\n    // Solve ODE (simplified)\n    real S = S0 * exp(-beta * I0 * time[i]);\n    real I = I0 * exp((beta * S0 - gamma) * time[i]);\n    cases[i] ~ normal(I * 1000, sigma);\n  }\n}\n\"\n```\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Particle Filtering\n\n**<span style=\"color: blue;\">Core Idea:</span>** Sequential Monte Carlo method for state-space models\n\n**When to Use:**\n\n-   Real-time parameter estimation\n-   State estimation in stochastic models\n-   Handling of missing data\n-   Time-varying parameters\n\n------------------------------------------------------------------------\n\n## Advantages\n\n-   Handles stochasticity naturally\n-   Real-time updates\n-   No assumption of constant parameters\n-   Robust to model misspecification\n\n------------------------------------------------------------------------\n\n## Example Application\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Particle filter for SIR model\nlibrary(pomp)\n\n# Define SIR model with stochasticity\nsir_pomp <- pomp(\n  data = data.frame(time = covid_times, cases = covid_observed),\n  times = \"time\",\n  t0 = 0,\n  rprocess = euler.sim(\n    step.fun = \"sir_step\",\n    delta.t = 0.1\n  ),\n  rmeasure = \"cases_measure\",\n  dmeasure = \"cases_dmeasure\",\n  initializer = \"sir_init\",\n  paramnames = c(\"beta\", \"gamma\", \"sigma\"),\n  statenames = c(\"S\", \"I\", \"R\")\n)\n\n# Run particle filter\npf <- pfilter(sir_pomp, Np = 1000, params = c(beta = 0.3, gamma = 0.1, sigma = 1))\n```\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Approximate Bayesian Computation (ABC)\n\n**<span style=\"color: blue;\">Core Idea:</span>** Approximate posterior without likelihood evaluation\n\n**When to Use:**\n\n-   Complex likelihoods\n-   Intractable models\n-   High-dimensional parameter spaces\n-   Model comparison\n\n------------------------------------------------------------------------\n\n**Algorithm:**\n\n1.  Sample parameters from prior\n2.  Simulate data from model\n3.  Compare simulated to observed data\n4.  Accept if distance \\< threshold\n\n------------------------------------------------------------------------\n\n**Advantages:**\n\n-   No likelihood required\n-   Handles complex models\n-   Model comparison\n-   Intuitive approach\n\n------------------------------------------------------------------------\n\n## Ensemble Methods\n\n**<span style=\"color: blue;\">Core Idea:</span>** Combine multiple models or methods\n\n**Types:**\n\n-   **Model Ensembles**: Average predictions from different models\n-   **Method Ensembles**: Combine LS, MLE, MCMC results\n-   **Bootstrap Ensembles**: Multiple fits with resampled data\n\n------------------------------------------------------------------------\n\n## Advantages:\n\n-   Reduces overfitting\n-   Quantifies model uncertainty\n-   More robust predictions\n-   Handles model selection uncertainty\n\n# Part VII: Practical Considerations\n\n------------------------------------------------------------------------\n\n## Common Fitting Problems\n\n**Convergence Issues:**\n\n-   Poor starting values\n-   Flat likelihood surfaces\n-   Numerical instabilities\n-   Parameter bounds\n\n------------------------------------------------------------------------\n\n**Identifiability Problems:**\n\n-   Multiple solutions\n-   Correlated parameters\n-   Insufficient data\n-   Model overparameterization\n\n------------------------------------------------------------------------\n\n**Solutions:**\n\n-   Multiple starting points\n-   Profile likelihood\n-   Data augmentation\n\n------------------------------------------------------------------------\n\n## Troubleshooting Guide\n\n**If Optimization Fails:**\n\n1.  Check starting values\n2.  Verify parameter bounds\n3.  Examine objective function\n4.  Try different algorithms\n5.  Simplify the model\n\n------------------------------------------------------------------------\n\n**If Parameters Are Unidentifiable:**\n\n1.  Fix some parameters\n2.  Use additional data\n3.  Add regularization\n4.  Consider model reduction\n5.  Use prior information\n\n------------------------------------------------------------------------\n\n**If Results Are Unrealistic:**\n\n1.  Check model assumptions\n2.  Verify data quality\n3.  Examine residuals\n4.  Test sensitivity\n5.  Consider alternative models\n\n------------------------------------------------------------------------\n\n## Best Practices\n\n**Before Fitting:**\n\n-   Understand your data\n-   Check model assumptions\n-   Set realistic parameter bounds\n-   Prepare multiple starting values\n\n------------------------------------------------------------------------\n\n**During Fitting:**\n\n-   Monitor convergence\n-   Check for local minima\n-   Validate results\n-   Document everything\n\n------------------------------------------------------------------------\n\n**After Fitting:**\n\n-   Assess goodness of fit\n-   Quantify uncertainty\n-   Test sensitivity\n-   Validate predictions\n\n------------------------------------------------------------------------\n\n## Software Recommendations\n\n**R Packages:**\n\n-   `bbmle`: Maximum likelihood\n-   `rstan`: Bayesian inference\n-   `pomp`: Particle filtering\n\n------------------------------------------------------------------------\n\n**Specialized Software:**\n\n-   `Stan`: Probabilistic programming\n-   `JAGS`: Bayesian analysis\n-   `PyMC`: Bayesian inference\n\n# Part VIII: Conclusions\n\n------------------------------------------------------------------------\n\n## Key Takeaways\n\n**Least Squares:**\n\n-   Fast and intuitive\n-   Good for exploration\n-   Limited uncertainty quantification\n-   Sensitive to assumptions\n\n------------------------------------------------------------------------\n\n**Maximum Likelihood:**\n\n-   Principled statistical framework\n-   Natural uncertainty quantification\n-   Enables model comparison\n-   More computationally intensive\n\n------------------------------------------------------------------------\n\n**Advanced Methods:**\n\n-   Handle complex scenarios\n-   Provide robust uncertainty\n-   Require more expertise\n-   Often computationally expensive\n\n------------------------------------------------------------------------\n\n## Choosing the Right Method\n\n**For Quick Exploration:** Least Squares\n\n**For Publication:** Maximum Likelihood\n\n**For Complex Models:** Bayesian Methods\n\n**For Real-time:** Particle Filtering\n\n**For Model Comparison:** ABC or MCMC\n\n**<span style=\"color: tomato;\">General Principle:</span>** Start simple, add complexity as needed\n\n------------------------------------------------------------------------\n\n## Final Thoughts\n\n**<span style=\"color: tomato;\">Model fitting is both art and science:</span>**\n\n-   Requires domain expertise\n-   Demands statistical rigor\n-   Benefits from computational tools\n-   Needs careful validation\n\n------------------------------------------------------------------------\n\n**<span style=\"color: blue;\">The goal is not just to fit models, but to:</span>**\n\n-   Understand disease dynamics\n-   Make reliable predictions\n-   Inform policy decisions\n-   Advance scientific knowledge\n\n------------------------------------------------------------------------\n\n::: {.fragment style=\"font-size: 450%\"}\nAny Questions?\n:::\n\n## References\n\n-   Anderson, R.M. and May, R.M. (1991). *Infectious Diseases of Humans: Dynamics and Control*. Oxford University Press.\n-   Keeling, M.J. and Rohani, P. (2008). *Modeling Infectious Diseases in Humans and Animals*. Princeton University Press.\n-   Bolker, B. (2008). *Ecological Models and Data in R*. Princeton University Press.\n-   King, A.A. et al. (2016). \"Statistical inference for partially observed Markov processes via the R package pomp.\" *Journal of Statistical Software*.\n-   Carpenter, B. et al. (2017). \"Stan: A probabilistic programming language.\" *Journal of Statistical Software*.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}